{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4088aa80-1951-4155-a9f0-1985add49e70",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b730448-cbed-41fb-af98-e1aa5d485130",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "import pickle\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import time, os\n",
    "import re\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "chromedriver = \"/Applications/chromedriver\" \n",
    "os.environ[\"webdriver.chrome.driver\"] = chromedriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d04ec6-49aa-4096-973e-dac2c1034e64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# request the html from the website\n",
    "\n",
    "url = 'https://www.cityrealty.com/nyc/apartments-for-rent/search-results#?page=1'\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(response.text[:300])\n",
    "else: \n",
    "    print(f'opps! Received status code {status}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e662a04-bce1-4c49-9a74-3714776882bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# I searched any apartments in NYC. Each search result page shows around 50 apartments. \n",
    "# so I need to loop through the pages to get 1000+ apartments.\n",
    "\n",
    "results_page = 'https://www.cityrealty.com/nyc/apartments-for-rent/search-results#?page='\n",
    "apt_page = []\n",
    "\n",
    "for page in range (1, 51):\n",
    "    driver = webdriver.Chrome(chromedriver)\n",
    "    driver.get(results_page + str(page))\n",
    "    soup = bs(driver.page_source, 'html.parser')\n",
    "    for link in soup.find_all('a', class_ = \"ng-scope\"):\n",
    "        if re.match('^/nyc/', link['href']):\n",
    "            apt_page.append(link['href'])\n",
    "    \n",
    "    driver.close()\n",
    "    time.sleep(.5+2*random.random())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbe673e4-d6a4-479d-84b4-22fcaa189d4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1500"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(apt_page) # shows how many apartment pages are in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "875b905f-7112-4b25-9580-0ec297eb9f54",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1430"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some of apartment in jersey city or hoboken nearby nyc are included so let's drop those.\n",
    "\n",
    "for i in apt_page:\n",
    "    if 'jersey-city' in i:\n",
    "        apt_page.remove(i)\n",
    "    elif 'hoboken' in i:\n",
    "        apt_page.remove(i)\n",
    "        \n",
    "len(apt_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c302742-a86d-4e97-92a2-87dc45b002bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle it in case I need the list of apartment in the future. \n",
    "# with open('apt_page.pickle', 'wb') as apt_page_list:\n",
    "#     pickle.dump(apt_page, apt_page_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b524d08-ac9e-433d-be5e-f6a7e32cd4c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let's scrape each page for every single apartment\n",
    "apt_pipeline = []\n",
    "\n",
    "url_base = 'https://www.cityrealty.com'\n",
    "url_list = [url_base + str(i) for i in apt_page]\n",
    "\n",
    "for url in url_list:\n",
    "    driver = webdriver.Chrome(chromedriver)\n",
    "    driver.get(url)\n",
    "    soup = bs(driver.page_source, 'html.parser')\n",
    "    \n",
    "    # each apartment page may show a subscribe pop-up page which prevents from scraping the data at first hand.\n",
    "    # if the subscribe page exists, I need to click the close buttom to remove that pop up screen.\n",
    "    close = driver.find_element_by_xpath('//*[@id=\"lst\"]/registration/div[2]/div[1]/i')\n",
    "    \n",
    "    if close.is_displayed():\n",
    "        close.click()\n",
    "    else:\n",
    "        None\n",
    "        \n",
    "    \n",
    "    if soup.find('title') == None:\n",
    "        address = 'missing'\n",
    "    else: address = soup.find('title').text.split(',')[0]\n",
    "    \n",
    "    if soup.find('span', class_ = \"_content _price\") == None:\n",
    "        price = 'missing'\n",
    "    else: price = soup.find('span', class_ = \"_content _price\").text.replace('$','')\\\n",
    "                .replace(',','').split('\\n')[1]\n",
    "    \n",
    "    if soup.find('span', class_= \"_content _beds\") == None:\n",
    "        num_beds = 'missing'\n",
    "    else: num_beds = soup.find('span', class_= \"_content _beds\").text.replace('\\n', '').split(' ')[0]\n",
    "    \n",
    "    if soup.find('span', class_= \"_content _baths\") == None:\n",
    "        num_baths = 'missing'\n",
    "    else: num_baths = soup.find('span', class_= \"_content _baths\").text\\\n",
    "                      .replace('\\n', '').replace(',', '').split(' ')[1]\n",
    "    \n",
    "    if soup.find('span', class_ = \"_option\") == None:\n",
    "        sq_ft = 'missing'\n",
    "    else: sq_ft = soup.find('span', class_ = \"_option\").next.next.replace(' ft', '')\n",
    "\n",
    "    if soup.find('span', class_ = \"_init\") == None:\n",
    "        listed_date = 'missing'\n",
    "    else: listed_date = soup.find('span', class_ = \"_init\").text.replace('Listed ', '')\n",
    "    \n",
    "    if soup.find('i', class_ = \"fa fa-map-signs\") == None:\n",
    "        neighborhood = 'missing'\n",
    "    else: neighborhood = soup.find('i', class_ = \"fa fa-map-signs\")\\\n",
    "            .next.next.next.text.replace('\\n', '')\n",
    "    \n",
    "    if soup.find('i', class_ = \"fa fa-building-o big\") == None:\n",
    "        fee = 'missing'\n",
    "    else: fee = soup.find('i', class_ = \"fa fa-building-o big\").next\n",
    "    \n",
    "    if soup.find('i', class_ = \"fa fa-wrench\") == None:\n",
    "        built_yr = 'missing'\n",
    "    else: built_yr = soup.find('i', class_ = \"fa fa-wrench\").next.split(' ')[2]\n",
    "        \n",
    "        \n",
    "    if soup.find('i', class_ = \"fa fa-key big\") == None:\n",
    "        unit = 'missing'\n",
    "    else: unit = soup.find('i', class_ = \"fa fa-key big\").next.split(' ')[0]\n",
    "        \n",
    "    if soup.find('li', class_ = \"last\") == None:\n",
    "        floors = 'missing'\n",
    "    else: floors = soup.find('li', class_ = \"last\").text.split(' ')[0]\n",
    "    \n",
    "    if soup.find('span', class_ = 'value') == None:\n",
    "        listed_date = 'missing'\n",
    "    else: listed_date = soup.find('span', class_ = 'value').text\n",
    "    #parsing a list of amentities and number of amenities\n",
    "    \n",
    "    if soup.find('span', class_ = 'value') == None:\n",
    "        listed_date = 'missing'\n",
    "    else: listed_date = soup.find('span', class_ = 'value').text.replace('\\n', '')\n",
    "        \n",
    "    amenities = []\n",
    "    if soup.find('ul', class_ = 'w_list') == None:\n",
    "        amenity_root = 'missing'\n",
    "    else: amenity_root = soup.find('ul', class_ = 'w_list').text.split('\\n')\n",
    "    \n",
    "\n",
    "    if amenity_root == 'missing':\n",
    "        None\n",
    "    else:\n",
    "        while \"\" in amenity_root:\n",
    "            amenity_root.remove(\"\")\n",
    "    \n",
    "    if amenity_root == 'missing':\n",
    "        None\n",
    "    else: amenity_list = [amenity.strip(' ') for amenity in amenity_root]\n",
    "    \n",
    "    \n",
    "    num_amenities = 0\n",
    "    \n",
    "    if amenity_root == 'missing':\n",
    "        num_amenities = 0\n",
    "    else:\n",
    "        for i in amenity_list:\n",
    "            num_amenities = num_amenities +1\n",
    "    \n",
    "    if soup.find(\"span\", class_ = \"distance\") == None:\n",
    "        closest_dist = 'missing'\n",
    "    else: closest_dist = soup.find(\"span\", class_ = \"distance\").text.split(' ')[0]\n",
    "        \n",
    "    apt_pipeline.append({'address': address, 'price': price, 'num_beds': num_beds,\n",
    "                     'num_baths': num_baths, 'sq_ft': sq_ft, 'listed_date': listed_date,\n",
    "                     'neighborhood': neighborhood,\n",
    "                     'fee': fee, 'built_yr': built_yr, 'unit': unit,\n",
    "                     'floors': floors, 'num_amenities': num_amenities,\n",
    "                     'closest_dist_station': closest_dist, 'url':url}) \n",
    "    \n",
    "    driver.close()\n",
    "    time.sleep(.5+2*random.random()) # this breaks some time between loading each page.        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5df73c8-bf1a-476f-ae18-a51824f1d385",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "apt_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccf3009-0daf-4363-9aca-a2784a2e8e08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline_df = pd.DataFrame(apt_pipeline)\n",
    "\n",
    "# for col in pipeline_df.columns:\n",
    "#   pipeline_df.loc[0, col] in pipeline_df[col].values  #first required post\n",
    "#   pipeline_df.loc[999, col] in pipeline_df[col].values #last required post\n",
    "\n",
    "pipeline_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b835c01-d8c6-4208-9859-919546d3f79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save as a csv file \n",
    "pipeline_df.to_csv('apt_info2.csv', sep='\\t', encoding='utf-8', header='true')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
